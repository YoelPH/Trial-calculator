{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import lymph\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy.special import factorial\n",
    "import matplotlib.pyplot as plt\n",
    "import emcee                      # inference and backends for sample storage\n",
    "from multiprocessing import Pool  # for parallelization of the inference\n",
    "\n",
    "\n",
    "dataset_full = pd.read_csv(\"../lynference/data/cleaned.csv\", header=[0,1,2]) #import data\n",
    "dataset_USZ =  pd.read_csv(\"../lynference/data/cleanedUSZ.csv\", header=[0,1,2]) #import data\n",
    "\n",
    "maxllh =  dataset_USZ['max_llh']\n",
    "t_stage = dataset_USZ['info']\n",
    "ipsi = maxllh.loc[:,'ipsi'].drop(['IIa','IIb','VIII','Ib','IX','VI','X','Ia'],axis = 1)[['I','II','III','IV','V','VII']]\n",
    "contra = maxllh.loc[:,'contra'].drop(['IIa','IIb','VIII','Ib','IX','VI','X','Ia'],axis = 1)[['I','II','III','IV','V','VII']]\n",
    "ipsi_header = header = pd.MultiIndex.from_product([ ['ipsi'], ['I','II','III','IV','V','VII']], names=['', ''])\n",
    "contra_header = pd.MultiIndex.from_product([['contra'], ['I','II','III','IV','V','VII']], names=['', ''])\n",
    "ipsi.columns = ipsi_header\n",
    "contra.columns = contra_header\n",
    "\n",
    "dataset_analyze = pd.concat([t_stage,ipsi,contra],axis = 1)\n",
    "\n",
    "CLB_full = dataset_full.loc[287:]\n",
    "maxllh_CLB =  CLB_full['max_llh']\n",
    "t_stage_CLB = CLB_full['info']\n",
    "ipsi_CLB = maxllh_CLB.loc[:,'ipsi'].drop(['IIa','IIb','VIII','Ib','IX','VI','X','Ia'],axis = 1)[['I','II','III','IV','V','VII']]\n",
    "contra_CLB = maxllh_CLB.loc[:,'contra'].drop(['IIa','IIb','VIII','Ib','IX','VI','X','Ia'],axis = 1)[['I','II','III','IV','V','VII']]\n",
    "ipsi_header = header = pd.MultiIndex.from_product([ ['ipsi'], ['I','II','III','IV','V','VII']], names=['', ''])\n",
    "contra_header = pd.MultiIndex.from_product([['contra'], ['I','II','III','IV','V','VII']], names=['', ''])\n",
    "ipsi_CLB.columns = ipsi_header\n",
    "contra_CLB.columns = contra_header\n",
    "\n",
    "dataset_CLB = pd.concat([t_stage_CLB,ipsi_CLB,contra_CLB],axis = 1)\n",
    "\n",
    "maxllh_full =  dataset_full['max_llh']\n",
    "t_stage_full = dataset_full['info']\n",
    "ipsi_full = maxllh_full.loc[:,'ipsi'].drop(['IIa','IIb','VIII','Ib','IX','VI','X','Ia'],axis = 1)[['I','II','III','IV','V','VII']]\n",
    "contra_full = maxllh_full.loc[:,'contra'].drop(['IIa','IIb','VIII','Ib','IX','VI','X','Ia'],axis = 1)[['I','II','III','IV','V','VII']]\n",
    "ipsi_header = header = pd.MultiIndex.from_product([ ['ipsi'], ['I','II','III','IV','V','VII']], names=['', ''])\n",
    "contra_header = pd.MultiIndex.from_product([['contra'], ['I','II','III','IV','V','VII']], names=['', ''])\n",
    "ipsi_full.columns = ipsi_header\n",
    "contra_full.columns = contra_header\n",
    "\n",
    "analysis_full = pd.concat([t_stage_full,ipsi_full,contra_full],axis = 1)\n",
    "analysis_full.fillna(False, inplace=True)\n",
    "(analysis_full['contra']['II'] == True).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(CLB_full['max_llh']['contra']['VII'] == True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = {\n",
    "    ('tumor', 'primary')  : ['I','II', 'III', 'IV','V', 'VII'],\n",
    "    ('lnl'  , 'I') :        [],\n",
    "    ('lnl'  , 'II') :       ['I','III','V'], \n",
    "    ('lnl'  , 'III'):       ['IV','V'], \n",
    "    ('lnl'  , 'IV') :       [],\n",
    "    ('lnl'  , 'V') :        [],\n",
    "    ('lnl'  , 'VII') :      [],\n",
    "}\n",
    "\n",
    "model = lymph.MidlineBilateral(graph = graph,use_mixing= True, trans_symmetric =True)\n",
    "model.modalities = {'CT': [0.76, 0.81],\n",
    "                    'MRI': [0.63, 0.81],\n",
    "                    'PET': [0.86, 0.79],\n",
    "                    'FNA': [0.98, 0.80],\n",
    "                    'diagnostic_consensus': [0.86, 0.81],\n",
    "                    'pathology': [1.0, 1.0],\n",
    "                    'pCT': [0.86, 0.81],\n",
    "                    'max_llh': [1.0, 1.0]\n",
    "                    }\n",
    "\n",
    "\n",
    "# Time prior with p(early) = 0.3\n",
    "def binom_pmf(k: np.ndarray, n: int, p: float):\n",
    "    \"\"\"Binomial PMF\"\"\"\n",
    "    if p > 1. or p < 0.:\n",
    "        raise ValueError(\"Binomial prob must be btw. 0 and 1\")\n",
    "    q = (1. - p)\n",
    "    binom_coeff = factorial(n) / (factorial(k) * factorial(n - k))\n",
    "    return binom_coeff * p**k * q**(n - k)\n",
    "\n",
    "def parametric_binom_pmf(n: int):\n",
    "    \"\"\"Return a parametric binomial PMF\"\"\"\n",
    "    def inner(t, p):\n",
    "        \"\"\"Parametric binomial PMF\"\"\"\n",
    "        return binom_pmf(t, n, p)\n",
    "    return inner\n",
    "\n",
    "max_t = 10\n",
    "model.diag_time_dists[\"early\"] = sp.stats.binom.pmf(np.arange(max_t+1), max_t, 0.3)\n",
    "model.diag_time_dists[\"late\"] = parametric_binom_pmf(max_t)\n",
    "model.patient_data = dataset_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base symmetric central\n",
    "graph = {\n",
    "    ('tumor', 'primary')  : ['I','II', 'III', 'IV','V', 'VII'],\n",
    "    ('lnl'  , 'I') :        [],\n",
    "    ('lnl'  , 'II') :       ['I','III','V'], \n",
    "    ('lnl'  , 'III'):       ['IV','V'], \n",
    "    ('lnl'  , 'IV') :       [],\n",
    "    ('lnl'  , 'V') :        [],\n",
    "    ('lnl'  , 'VII') :      [],\n",
    "}\n",
    "\n",
    "central = lymph.Bilateral(graph = graph,base_symmetric= True, trans_symmetric= True )\n",
    "central.modalities = {'CT': [0.76, 0.81],\n",
    "                    'MRI': [0.63, 0.81],\n",
    "                    'PET': [0.86, 0.79],\n",
    "                    'FNA': [0.98, 0.80],\n",
    "                    'diagnostic_consensus': [0.86, 0.81],\n",
    "                    'pathology': [1.0, 1.0],\n",
    "                    'pCT': [0.86, 0.81],\n",
    "                    'max_llh': [1.0, 1.0]\n",
    "                    }\n",
    "\n",
    "\n",
    "# Time prior with p(early) = 0.3\n",
    "def binom_pmf(k: np.ndarray, n: int, p: float):\n",
    "    \"\"\"Binomial PMF\"\"\"\n",
    "    if p > 1. or p < 0.:\n",
    "        raise ValueError(\"Binomial prob must be btw. 0 and 1\")\n",
    "    q = (1. - p)\n",
    "    binom_coeff = factorial(n) / (factorial(k) * factorial(n - k))\n",
    "    return binom_coeff * p**k * q**(n - k)\n",
    "\n",
    "def parametric_binom_pmf(n: int):\n",
    "    \"\"\"Return a parametric binomial PMF\"\"\"\n",
    "    def inner(t, p):\n",
    "        \"\"\"Parametric binomial PMF\"\"\"\n",
    "        return binom_pmf(t, n, p)\n",
    "    return inner\n",
    "\n",
    "max_t = 10\n",
    "central.diag_time_dists[\"early\"] = sp.stats.binom.pmf(np.arange(max_t+1), max_t, 0.3)\n",
    "central.diag_time_dists[\"late\"] = parametric_binom_pmf(max_t)\n",
    "central.patient_data = dataset_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "central.modalities = {'max_llh_diagnose' : [1,0.81]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = emcee.backends.HDFBackend(filename = \"../lynference/models/samples.hdf5\")\n",
    "samples = backend.get_chain(flat = True)\n",
    "model.check_and_assign(samples.mean(axis = 0))\n",
    "model.modalities = {'max_llh_diagnose' : [1,0.81]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00979183, 0.3781374 , 0.06128133, 0.00958199, 0.00848541,\n",
       "       0.02088659, 0.03926396, 0.16041497, 0.01144267, 0.16367662,\n",
       "       0.04831059, 0.38639099])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilateral_samples = np.zeros((19000,12))\n",
    "bilateral_samples[:,0:6] = samples[:,0:6]\n",
    "bilateral_samples[:,6:] = samples[:,13:]\n",
    "bilateral_samples.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_sampled_bilateral(samples, model, t_stage, given_diagnoses, thin = 89):\n",
    "    sampled_risks = np.zeros(shape=(len(samples[::thin]),64,64), dtype=float)\n",
    "    for i, sample in enumerate(np.random.permutation(samples[::thin])):\n",
    "        sampled_risks[i] = model.risk(given_params = sample, t_stage = t_stage, given_diagnoses = given_diagnoses) \n",
    "    mean_risk = sampled_risks.mean(axis = 0)\n",
    "    return sampled_risks, mean_risk\n",
    "\n",
    "def levels_to_spare_bilateral(threshold, model, risks, sampled_risks):\n",
    "    \"\"\"Computes which LNLs to irradiate given the threshold, model and the risk of each state.\n",
    "\n",
    "    Args:\n",
    "        threshold (float): Risk threshold we want to apply\n",
    "        model (lymph.Unilateral): lymph.unilateral object with fully analyzed patients\n",
    "        risks (ndarray): Array with the risk of each state\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    state_list = model.ipsi.state_list\n",
    "    lnls = ['I', 'II', 'III', 'IV', 'V', 'VII']\n",
    "    overall_risk_ipsi = {}\n",
    "    overall_risk_contra = {}\n",
    "    for index, lnl in enumerate(lnls):\n",
    "        overall_risk_ipsi[lnl] = risks[np.where((state_list[:,index] == 1))[0]].sum()\n",
    "        overall_risk_contra[lnl] = risks.T[np.where((state_list[:,index] == 1))[0]].sum()\n",
    "\n",
    "    combined_dict = {f'ipsi {key}': value for key, value in overall_risk_ipsi.items()}\n",
    "    combined_dict.update({f'contra {key}': value for key, value in overall_risk_contra.items()})\n",
    "    ranked_combined = sorted(combined_dict.items(), key = lambda item: item[1])\n",
    "    total_risk_new = 0\n",
    "    sampled_total_risks_new = np.zeros(sampled_risks.shape[0])\n",
    "    looper = 1\n",
    "    treated_array = np.ones(12)\n",
    "    contra_lnl_indices = []\n",
    "    ipsi_lnl_indices = []\n",
    "    treated_ipsi = []\n",
    "    treated_contra = []\n",
    "    while total_risk_new < threshold:\n",
    "        sampled_total_risks = sampled_total_risks_new\n",
    "        total_risk = total_risk_new\n",
    "        if ipsi_lnl_indices != []:\n",
    "            treated_array[ipsi_lnl_indices] = 0\n",
    "        if contra_lnl_indices != []:\n",
    "            treated_array[np.array(contra_lnl_indices)+6] = 0\n",
    "        lnls_of_interest = ranked_combined[0:looper]\n",
    "        lnls_of_interest_names = [t[0] for t in lnls_of_interest]\n",
    "        contra_lnl_indices = []\n",
    "        ipsi_lnl_indices = []\n",
    "        for i,lnl_looper in enumerate(lnls_of_interest_names):\n",
    "            contra_lnl_indices.append(np.where(np.array(lnls) == lnls_of_interest_names[i].split()[1])[0][0]) if lnl_looper.split()[0] == 'contra' else ipsi_lnl_indices.append(np.where(np.array(lnls) == lnls_of_interest_names[i].split()[1])[0][0])\n",
    "        indices_list_contra = []\n",
    "        indices_list_ipsi = []\n",
    "        for index in contra_lnl_indices:\n",
    "            condition_contra = (state_list[:, index] == 1)\n",
    "            indices_contra = np.where(condition_contra)[0]\n",
    "            indices_list_contra.extend(indices_contra)\n",
    "            unique_contra = np.unique(indices_list_contra)\n",
    "        for index in ipsi_lnl_indices:\n",
    "            condition_ipsi = (state_list[:, index] == 1)\n",
    "            indices_ipsi = np.where(condition_ipsi)[0]\n",
    "            indices_list_ipsi.extend(indices_ipsi)\n",
    "            unique_ipsi = np.unique(indices_list_ipsi)    \n",
    "        if len(ipsi_lnl_indices) == 0:\n",
    "            total_risk_new = risks.T[unique_contra].sum()\n",
    "            sampled_total_risks_new = sampled_risks.transpose((0,2,1))[:,unique_contra].sum(axis = (1,2))\n",
    "        elif len(contra_lnl_indices) == 0:\n",
    "            total_risk_new = risks[unique_ipsi].sum()\n",
    "            sampled_total_risks_new = sampled_risks[:,unique_ipsi].sum(axis = (1,2))\n",
    "        else:\n",
    "            total_risk_new = 0\n",
    "            sampled_total_risks_new = np.zeros(sampled_risks.shape[0])\n",
    "            total_risk_new += risks[unique_ipsi].sum()\n",
    "            total_risk_new += risks.T[unique_contra][:,[np.setdiff1d(np.array(range(64)),unique_ipsi)]].sum()\n",
    "            sampled_total_risks_new += sampled_risks[:,unique_ipsi].sum(axis = (1,2))\n",
    "            sampled_total_risks_new += sampled_risks.transpose((0,2,1))[:,unique_contra][:,:,list(np.setdiff1d(np.array(range(64)),unique_ipsi))].sum(axis = (1,2))\n",
    "\n",
    "        spared_lnls = lnls_of_interest[:-1]\n",
    "        treated_lnls = ranked_combined[looper-1:]\n",
    "        looper += 1\n",
    "    for to_treat in treated_lnls:\n",
    "        if to_treat[0].split()[0] == 'ipsi':\n",
    "            treated_ipsi.append(to_treat[0].split()[1])\n",
    "        else: \n",
    "            treated_contra.append(to_treat[0].split()[1])\n",
    "    return spared_lnls, total_risk, ranked_combined, treated_lnls, treated_array, treated_ipsi, treated_contra,sampled_total_risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">tumor</th>\n",
       "      <th colspan=\"6\" halign=\"left\">ipsi</th>\n",
       "      <th colspan=\"6\" halign=\"left\">contra</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>t_stage</th>\n",
       "      <th>midline_extension</th>\n",
       "      <th>I</th>\n",
       "      <th>II</th>\n",
       "      <th>III</th>\n",
       "      <th>IV</th>\n",
       "      <th>V</th>\n",
       "      <th>VII</th>\n",
       "      <th>I</th>\n",
       "      <th>II</th>\n",
       "      <th>III</th>\n",
       "      <th>IV</th>\n",
       "      <th>V</th>\n",
       "      <th>VII</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>late</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>late</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>late</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>early</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>early</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>late</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>early</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>late</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>late</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tumor                     ipsi                                     \\\n",
       "    t_stage midline_extension      I     II    III     IV      V    VII   \n",
       "18     late              True  False  False  False  False  False  False   \n",
       "79     late              True  False  False  False  False  False  False   \n",
       "87     late              True  False  False  False  False  False  False   \n",
       "91    early              True  False  False  False  False  False  False   \n",
       "144   early              True  False  False  False  False  False  False   \n",
       "150    late              True  False  False  False  False  False  False   \n",
       "178   early              True  False  False  False  False  False  False   \n",
       "183    late              True  False  False  False  False  False  False   \n",
       "207    late              True  False   True   True  False  False  False   \n",
       "\n",
       "    contra                                     \n",
       "         I     II    III     IV      V    VII  \n",
       "18   False  False  False  False  False  False  \n",
       "79   False   True  False  False  False  False  \n",
       "87   False  False  False  False  False  False  \n",
       "91   False  False  False  False  False  False  \n",
       "144  False  False  False  False  False  False  \n",
       "150  False  False  False  False  False  False  \n",
       "178  False  False  False  False  False  False  \n",
       "183  False  False  False  False  False  False  \n",
       "207  False   True   True  False  False  False  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_USZ =  pd.read_csv(\"../lynference/data/2021-usz-oropharynx.csv\", header=[0,1,2]) #import data\n",
    "central_patients = dataset_analyze.loc[full_USZ['tumor']['1']['central']]\n",
    "central_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ipsi V', 0.025064797870341552), ('ipsi I', 0.029081265446687262), ('ipsi IV', 0.05662126683553424), ('contra IV', 0.06006855628615346), ('contra II', 0.682505365057676), ('ipsi II', 0.9999999999999998), ('ipsi III', 0.9999999999999998), ('contra III', 0.9999999999999999)]\n",
      "8.054970616530472\n",
      "[('ipsi VII', 0.017820852057412438), ('contra VII', 0.01782085205741244), ('contra I', 0.023617219939726007), ('contra V', 0.024257871587631225)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6.73615544, 9.41561591])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ci_single(sampled_risks, level = 0.95):\n",
    "    lower = (1-level)/2*100\n",
    "    upper = 100- lower\n",
    "    ci = np.percentile(sampled_risks,[lower,upper])\n",
    "    return ci\n",
    "diagnose = {'max_llh_diagnose':{\n",
    "    \"ipsi\": {\n",
    "        \"I\": 0,\n",
    "        \"II\": 1,\n",
    "        \"III\": 1,\n",
    "        \"IV\": 0,\n",
    "        \"V\": 0,\n",
    "        \"VII\": 0,\n",
    "    },\n",
    "    \"contra\": {\n",
    "        \"I\": 0,\n",
    "        \"II\": 0,\n",
    "        \"III\": 1,\n",
    "        \"IV\": 0,\n",
    "        \"V\": 0,\n",
    "        \"VII\": 0,\n",
    "    }\n",
    "}}\n",
    "sampled_risks, risk = risk_sampled_bilateral(samples = bilateral_samples, model = central, t_stage = 'late', given_diagnoses= diagnose)     \n",
    "spared_lnls, total_risk, ranked_combined, treated_lnls, treated_array, treated_ipsi, treated_contra, sampled_total_risks = levels_to_spare_bilateral(0.10, central, risk, sampled_risks)\n",
    "print(treated_lnls)\n",
    "print(total_risk*100)\n",
    "print(spared_lnls)\n",
    "ci_single(sampled_total_risks)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.where((central.ipsi.state_list[:,3] == 1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05662126683553427"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk[indices].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.060068556286153464"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk.T[indices].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "central.risk().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code extracts the risks from our samples. thin was set to 89 which results in 214 samples/risks taken. If this number is changed, some downstream codes need to be adapted.\n",
    "The mean_risk is simply the mean of the sampled risks. --> used for ranking later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_sampled(samples, model, t_stage, given_diagnoses, midline_extension, thin = 89):\n",
    "    sampled_risks = np.zeros(shape=(len(samples[::thin]),64,64), dtype=float)\n",
    "    for i, sample in enumerate(np.random.permutation(samples[::thin])):\n",
    "        sampled_risks[i] = model.risk(given_params = sample, t_stage = t_stage, given_diagnoses = given_diagnoses,midline_extension=midline_extension) \n",
    "    mean_risk = sampled_risks.mean(axis = 0)\n",
    "    return sampled_risks, mean_risk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute which levels are to be spared based on given mean risks, or sampled risks. The ranking is based on the mean risk of each state (which is equivalent to the mean risk when directly sampling the LNL involvement risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levels_to_spare(threshold, model, risks, sampled_risks = None):\n",
    "    \"\"\"Computes which LNLs to irradiate given the threshold, model and the risk of each state.\n",
    "\n",
    "    Args:\n",
    "        threshold (float): Risk threshold we want to apply\n",
    "        model (lymph.Midline): lymph.Midline object with fully analyzed patients\n",
    "        risks (ndarray): Array with the risk of each state\n",
    "        sampled_risks (ndarray): Array with sampled risks. i.e. Simply an n times risks.shape dimensional array holding several risks calculations\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    sampled_total_risks = None\n",
    "    #extract the state list. adapt to the model that is used!\n",
    "    state_list = model.noext.ipsi.state_list\n",
    "    lnls = []\n",
    "    for lnl in model.noext.ipsi.lnls:\n",
    "        lnls.append(lnl.name)\n",
    "    overall_risk_ipsi = {}\n",
    "    overall_risk_contra = {}\n",
    "    #compute the risk for each lnl with the averaged risk array\n",
    "    for index, lnl in enumerate(lnls):\n",
    "        overall_risk_ipsi[lnl] = risks[np.where((state_list[:,index] == 1))[0]].sum()\n",
    "        overall_risk_contra[lnl] = risks.T[np.where((state_list[:,index] == 1))[0]].sum()\n",
    "    #combine both dictionaries from ipsi and contra to rank the lnls together\n",
    "    combined_dict = {f'ipsi {key}': value for key, value in overall_risk_ipsi.items()}\n",
    "    combined_dict.update({f'contra {key}': value for key, value in overall_risk_contra.items()})\n",
    "    ranked_combined = sorted(combined_dict.items(), key = lambda item: item[1])\n",
    "\n",
    "    #here we start extracting the risk for each lnl. with every loop we exclude a LNL from irradiation until we hit the threshold\n",
    "    total_risk_new = 0\n",
    "    if sampled_risks is not None and sampled_risks.size > 0:\n",
    "\n",
    "        sampled_total_risks_new = np.zeros(sampled_risks.shape[0])\n",
    "    looper = 1\n",
    "    treated_array = np.ones(12)\n",
    "    contra_lnl_indices = []\n",
    "    ipsi_lnl_indices = []\n",
    "    treated_ipsi = []\n",
    "    treated_contra = []\n",
    "    while total_risk_new < threshold:\n",
    "        if sampled_risks is not None and sampled_risks.size > 0:\n",
    "\n",
    "            sampled_total_risks = sampled_total_risks_new\n",
    "        total_risk = total_risk_new\n",
    "        if ipsi_lnl_indices != []:\n",
    "            treated_array[ipsi_lnl_indices] = 0\n",
    "        if contra_lnl_indices != []:\n",
    "            treated_array[np.array(contra_lnl_indices)+6] = 0\n",
    "        lnls_of_interest = ranked_combined[0:looper]\n",
    "        lnls_of_interest_names = [t[0] for t in lnls_of_interest]\n",
    "        contra_lnl_indices = []\n",
    "        ipsi_lnl_indices = []\n",
    "        for i,lnl_looper in enumerate(lnls_of_interest_names):\n",
    "            contra_lnl_indices.append(np.where(np.array(lnls) == lnls_of_interest_names[i].split()[1])[0][0]) if lnl_looper.split()[0] == 'contra' else ipsi_lnl_indices.append(np.where(np.array(lnls) == lnls_of_interest_names[i].split()[1])[0][0])\n",
    "        indices_list_contra = []\n",
    "        indices_list_ipsi = []\n",
    "        for index in contra_lnl_indices:\n",
    "            condition_contra = (state_list[:, index] == 1)\n",
    "            indices_contra = np.where(condition_contra)[0]\n",
    "            indices_list_contra.extend(indices_contra)\n",
    "            unique_contra = np.unique(indices_list_contra)\n",
    "        for index in ipsi_lnl_indices:\n",
    "            condition_ipsi = (state_list[:, index] == 1)\n",
    "            indices_ipsi = np.where(condition_ipsi)[0]\n",
    "            indices_list_ipsi.extend(indices_ipsi)\n",
    "            unique_ipsi = np.unique(indices_list_ipsi)    \n",
    "        if len(ipsi_lnl_indices) == 0:\n",
    "            total_risk_new = risks.T[unique_contra].sum()\n",
    "            if sampled_risks is not None and sampled_risks.size > 0:\n",
    "\n",
    "                sampled_total_risks_new = sampled_risks.transpose((0,2,1))[:,unique_contra].sum(axis = (1,2))\n",
    "        elif len(contra_lnl_indices) == 0:\n",
    "            total_risk_new = risks[unique_ipsi].sum()\n",
    "            if sampled_risks is not None and sampled_risks.size > 0:\n",
    "\n",
    "                sampled_total_risks_new = sampled_risks[:,unique_ipsi].sum(axis = (1,2))\n",
    "        else:\n",
    "            total_risk_new = 0\n",
    "            total_risk_new += risks[unique_ipsi].sum()\n",
    "            total_risk_new += risks.T[unique_contra][:,[np.setdiff1d(np.array(range(64)),unique_ipsi)]].sum()\n",
    "            if sampled_risks is not None and sampled_risks.size > 0:\n",
    "\n",
    "                sampled_total_risks_new = np.zeros(sampled_risks.shape[0])\n",
    "                sampled_total_risks_new += sampled_risks[:,unique_ipsi].sum(axis = (1,2))\n",
    "                sampled_total_risks_new += sampled_risks.transpose((0,2,1))[:,unique_contra][:,:,list(np.setdiff1d(np.array(range(64)),unique_ipsi))].sum(axis = (1,2))\n",
    "\n",
    "        spared_lnls = lnls_of_interest[:-1]\n",
    "        treated_lnls = ranked_combined[looper-1:]\n",
    "        looper += 1\n",
    "    for to_treat in treated_lnls:\n",
    "        if to_treat[0].split()[0] == 'ipsi':\n",
    "            treated_ipsi.append(to_treat[0].split()[1])\n",
    "        else: \n",
    "            treated_contra.append(to_treat[0].split()[1])\n",
    "    return spared_lnls, total_risk, ranked_combined, treated_lnls, treated_array, treated_ipsi, treated_contra,sampled_total_risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07277896938230138"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnose = {'max_llh_diagnose':{\n",
    "    \"ipsi\": {\n",
    "        \"I\": 0,\n",
    "        \"II\": 1,\n",
    "        \"III\": 0,\n",
    "        \"IV\": 0,\n",
    "        \"V\": 0,\n",
    "        \"VII\": 0,\n",
    "    },\n",
    "    \"contra\": {\n",
    "        \"I\": 0,\n",
    "        \"II\": 0,\n",
    "        \"III\": 0,\n",
    "        \"IV\": 0,\n",
    "        \"V\": 0,\n",
    "        \"VII\": 0,\n",
    "    }\n",
    "}}\n",
    "involv = {'ipsi':{\n",
    "        \"I\": None,\n",
    "        \"II\": None,\n",
    "        \"III\": True,\n",
    "        \"IV\": None,\n",
    "        \"V\": None,\n",
    "        \"VII\": None,\n",
    "    },\n",
    "    \"contra\":{\n",
    "        \"I\": None,\n",
    "        \"II\": None,\n",
    "        \"III\": None,\n",
    "        \"IV\": None,\n",
    "        \"V\": None,\n",
    "        \"VII\": None,\n",
    "    }\n",
    "}\n",
    "model.risk(given_diagnoses= diagnose, involvement= involv, t_stage = 'early', midline_extension= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_test = model.risk(given_diagnoses= diagnose, t_stage = 'early', midline_extension= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "marg_states = {}   # vectors marginalizing over only the states we care about\n",
    "for side in [\"ipsi\", \"contra\"]:\n",
    "    if isinstance(involv[side], dict):\n",
    "        involv[side] = np.array(\n",
    "            [involv[side].get(lnl.name, None) for lnl in side_model.lnls]\n",
    "        )\n",
    "    else:\n",
    "        involv[side] = np.array(involv[side])\n",
    "\n",
    "    side_model = getattr(model.noext, side)\n",
    "    marg_states[side] = np.zeros(shape=len(side_model.state_list), dtype=bool)\n",
    "    for i,state in enumerate(side_model.state_list):\n",
    "        marg_states[side][i] = np.all(np.equal(\n",
    "            involv[side], state,\n",
    "            where=(involv[side] != None),\n",
    "            out=np.ones_like(state, dtype=bool)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07277896938230138"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marg_states[\"ipsi\"] @ risk_test @ marg_states[\"contra\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ipsi III', 0.07277896938230134), ('ipsi II', 1.0000000000000004)]\n",
      "6.534802885096467\n",
      "[('contra V', 0.00010966725001120429), ('contra I', 0.0004831298756013683), ('contra III', 0.0008017051638956271), ('contra IV', 0.001335568020490673), ('contra VII', 0.004077454222696565), ('ipsi IV', 0.00860530278722505), ('ipsi V', 0.010937710429865695), ('contra II', 0.01124515135799644), ('ipsi I', 0.013827957310150005), ('ipsi VII', 0.016958011416240093)]\n"
     ]
    }
   ],
   "source": [
    "diagnose = {'max_llh_diagnose':{\n",
    "    \"ipsi\": {\n",
    "        \"I\": 0,\n",
    "        \"II\": 1,\n",
    "        \"III\": 0,\n",
    "        \"IV\": 0,\n",
    "        \"V\": 0,\n",
    "        \"VII\": 0,\n",
    "    },\n",
    "    \"contra\": {\n",
    "        \"I\": 0,\n",
    "        \"II\": 0,\n",
    "        \"III\": 0,\n",
    "        \"IV\": 0,\n",
    "        \"V\": 0,\n",
    "        \"VII\": 0,\n",
    "    }\n",
    "}}\n",
    "sampled_risks, risk = risk_sampled(samples = samples, model = model, t_stage = 'early', given_diagnoses= diagnose, midline_extension= False)     \n",
    "spared_lnls, total_risk, ranked_combined, treated_lnls, treated_array, treated_ipsi, treated_contra, sampled_total_risks = levels_to_spare(0.10, model, risk_test, sampled_risks = sampled_risks)\n",
    "print(treated_lnls)\n",
    "print(total_risk*100)\n",
    "print(spared_lnls)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Sample array with different entry combinations\n",
    "data = np.array(dataset_analyze)\n",
    "\n",
    "entry_combinations_with_indexes = defaultdict(list)\n",
    "for index, row in enumerate(data):\n",
    "    combination = tuple(row)\n",
    "    entry_combinations_with_indexes[combination].append(index)\n",
    "USZ_counts = []\n",
    "USZ_combinations = []\n",
    "USZ_indexes = []\n",
    "# Print the most common combinations, their USZ_counts, and indexes\n",
    "for combination, indexes in entry_combinations_with_indexes.items():\n",
    "    count = len(indexes)\n",
    "    USZ_indexes.append(indexes)\n",
    "    # print(f\"Combination: {combination}, Count: {count}, Indexes: {indexes}\")\n",
    "    USZ_counts.append(count)\n",
    "    USZ_combinations.append(combination)\n",
    "\n",
    "lnls = ['I','II', 'III', 'IV','V', 'VII']\n",
    "t_stage = []\n",
    "midline_extension = []\n",
    "invovlvement_ipsi_USZ = []\n",
    "invovlvement_contra_USZ = []\n",
    "for diagnose_type in USZ_combinations:\n",
    "    involved_ipsi = []\n",
    "    involved_contra = []\n",
    "    t_stage.append(diagnose_type[0])\n",
    "    midline_extension.append(diagnose_type[1])\n",
    "    for lnl_looper, involved_level in enumerate(lnls):\n",
    "        if diagnose_type[lnl_looper +2] == True:\n",
    "            involved_ipsi.append(involved_level) \n",
    "        if diagnose_type[lnl_looper +8] == True:\n",
    "            involved_contra.append(involved_level)\n",
    "    invovlvement_ipsi_USZ.append(involved_ipsi)\n",
    "    invovlvement_contra_USZ.append(involved_contra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = pd.DataFrame(USZ_combinations)\n",
    "# df2.to_csv('involvement_combinations_USZ.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we havethe code to compute the treated lnls. The code samples the risk of involvement given a diagnose and outputs all samples. Which allows us to build confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_treated_lnls_sampled(combinations):\n",
    "    treatment_array = np.zeros((len(combinations),12))\n",
    "    top3_spared = []\n",
    "    diagnose_looper = {'max_llh_diagnose':{\n",
    "        \"ipsi\": {\n",
    "            \"I\": 0,\n",
    "            \"II\": 0,\n",
    "            \"III\": 0,\n",
    "            \"IV\": 0,\n",
    "            \"V\": 0,\n",
    "            \"VII\": 0,\n",
    "        },\n",
    "        \"contra\": {\n",
    "            \"I\": 0,\n",
    "            \"II\": 0,\n",
    "            \"III\": 0,\n",
    "            \"IV\": 0,\n",
    "            \"V\": 0,\n",
    "            \"VII\": 0,\n",
    "        }\n",
    "    }}\n",
    "    treated_lnls_all = []\n",
    "    treated_lnls_no_risk = []\n",
    "    total_risks = np.zeros(len(combinations))\n",
    "    sampled_risks_array = np.zeros((len(combinations),214))\n",
    "    treated_ipsi_all = []\n",
    "    treated_contra_all = []\n",
    "    for index, pattern in enumerate(combinations):\n",
    "        treated_looper = set()\n",
    "        stage = pattern[0]\n",
    "        midline_extension = pattern[1]\n",
    "        counter_ipsi = 0\n",
    "        for lnl_ipsi, status in diagnose_looper['max_llh_diagnose']['ipsi'].items():\n",
    "            diagnose_looper['max_llh_diagnose']['ipsi'][lnl_ipsi] = pattern[2+counter_ipsi]\n",
    "            counter_ipsi += 1\n",
    "        counter_contra = 0\n",
    "        for lnl_contra, status in diagnose_looper['max_llh_diagnose']['contra'].items():\n",
    "            diagnose_looper['max_llh_diagnose']['contra'][lnl_contra] = pattern[8+counter_contra]\n",
    "            counter_contra += 1\n",
    "        sampled_risks, mean_risk = risk_sampled(samples = samples, model = model, t_stage = stage, given_diagnoses=diagnose_looper,midline_extension=midline_extension)     \n",
    "        spared_lnls, total_risk, ranked_combined, treated_lnls, treated_array, treated_ipsi, treated_contra, sampled_total_risks =levels_to_spare(0.10, model, mean_risk, sampled_risks)\n",
    "        for i in treated_lnls:\n",
    "            treated_looper.add(i[0])\n",
    "        treated_lnls_all.append(treated_lnls)\n",
    "        treated_lnls_no_risk.append(treated_looper)\n",
    "        treatment_array[index] = treated_array\n",
    "        total_risks[index] = total_risk\n",
    "        sampled_risks_array[index] = sampled_total_risks\n",
    "        top3_spared.append(spared_lnls[::-1][:3])\n",
    "        treated_ipsi_all.append(treated_ipsi)\n",
    "        treated_contra_all.append(treated_contra)\n",
    "    return treated_lnls_no_risk, treated_lnls_all, treatment_array, top3_spared, total_risks, treated_ipsi_all, treated_contra_all, sampled_risks_array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def count_number_treatments(treated_lnls_no_risk):\n",
    "    set_counts = {}\n",
    "    # Iterate through the list and update the counts in the dictionary\n",
    "    for value in treated_lnls_no_risk:\n",
    "        frozen_set = frozenset(value)  # Convert the set to a frozenset\n",
    "        if frozen_set in set_counts:\n",
    "            set_counts[frozen_set] += 1\n",
    "        else:\n",
    "            set_counts[frozen_set] = 1\n",
    "    return set_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usz_treated_lnls_no_risk, usz_treated_lnls_all, usz_treatment_array, usz_top3_spared, usz_total_risks, usz_treated_ipsi, usz_treated_contra, usz_sampled_risks_array = analysis_treated_lnls_sampled(USZ_combinations)\n",
    "usz_set_counts = count_number_treatments(usz_treated_lnls_no_risk)\n",
    "len(usz_set_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code computes the CIs for our sampled risks by taking the equal tailed interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ci_calculator(sampled_risks,level = 0.95):\n",
    "    lower = (1-level)/2*100\n",
    "    upper = 100- lower\n",
    "    ci = np.zeros((77,2))\n",
    "    for index in range(77):\n",
    "        ci[index] = np.percentile(sampled_risks[index],[lower,upper])\n",
    "    return ci\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = ci_calculator(usz_sampled_risks_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(usz_treatment_array)\n",
    "df.to_csv('treatment_array_010_new.csv')\n",
    "\n",
    "data_export_usz = pd.DataFrame({'Percentage of patients': np.array(USZ_counts)/287,\n",
    "                                'T-stage': t_stage,\n",
    "                                'Midline Extension': midline_extension,\n",
    "                                'Involvement Ipsi' : invovlvement_ipsi_USZ,\n",
    "                                'Involvement Contra': invovlvement_contra_USZ,\n",
    "                                'Treated Ipsi':  usz_treated_ipsi,\n",
    "                                'Treated Contra': usz_treated_contra,\n",
    "                                'risk': usz_total_risks,\n",
    "                                'lower bound': ci.T[0],\n",
    "                                'upper bound': ci.T[1],\n",
    "                                'top 3 spared lnls risk': usz_top3_spared\n",
    "\n",
    "})\n",
    "data_export_usz.to_csv('analyzed_usz_data_010_new.csv', sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "number_of_repetitions = []\n",
    "for key, value in usz_set_counts.items():\n",
    "    number_of_repetitions.append(value)\n",
    "usz_set_counts[frozenset({'ipsi II'})]\n",
    "asdf = (np.array(USZ_combinations) == ['late',False,False,True,False,False,False,False,False,False,False,False,False,False,])\n",
    "# Define the condition (e.g., all 'True' values)\n",
    "condition = (asdf[:, 1:] == 'True').all(axis=1)\n",
    "\n",
    "# Find indices where the condition is met\n",
    "indices = np.where(condition)\n",
    "\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we repeat the analysis for the CLB dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample array with different entry combinations\n",
    "data_CLB = np.array(dataset_CLB)\n",
    "\n",
    "entry_combinations_with_indexes = defaultdict(list)\n",
    "for index, row in enumerate(data_CLB):\n",
    "    combination = tuple(row)\n",
    "    entry_combinations_with_indexes[combination].append(index)\n",
    "CLB_counts = []\n",
    "CLB_combinations = []\n",
    "CLB_indexes = []\n",
    "# Print the most common combinations, their CLB_counts, and indexes\n",
    "for combination, indexes in entry_combinations_with_indexes.items():\n",
    "    count = len(indexes)\n",
    "    CLB_indexes.append(indexes)\n",
    "    # print(f\"Combination: {combination}, Count: {count}, Indexes: {indexes}\")\n",
    "    CLB_counts.append(count)\n",
    "    CLB_combinations.append(combination)\n",
    "\n",
    "lnls = ['I','II', 'III', 'IV','V', 'VII']\n",
    "t_stage = []\n",
    "midline_extension = []\n",
    "involvement_ipsi_CLB = []\n",
    "involvement_contra_CLB = []\n",
    "for diagnose_type in CLB_combinations:\n",
    "    involved_ipsi = []\n",
    "    involved_contra = []\n",
    "    t_stage.append(diagnose_type[0])\n",
    "    midline_extension.append(diagnose_type[1])\n",
    "    for lnl_looper, involved_level in enumerate(lnls):\n",
    "        if diagnose_type[lnl_looper +2] == True:\n",
    "            involved_ipsi.append(involved_level) \n",
    "        if diagnose_type[lnl_looper +8] == True:\n",
    "            involved_contra.append(involved_level)\n",
    "    involvement_ipsi_CLB.append(involved_ipsi)\n",
    "    involvement_contra_CLB.append(involved_contra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clb_treated_lnls_no_risk, clb_treated_lnls_all, clb_treatment_array, clb_top3_spared, clb_total_risks, clb_treated_ipsi, clb_treated_contra, clb_sampled_risks_array = analysis_treated_lnls_sampled(CLB_combinations)\n",
    "clb_set_counts = count_number_treatments(clb_treated_lnls_no_risk)\n",
    "len(clb_set_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clb_treated_contra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ci_calculator(sampled_risks,level = 0.95):\n",
    "    lower = (1-level)/2*100\n",
    "    upper = 100- lower\n",
    "    ci = np.zeros((69,2))\n",
    "    for index in range(69):\n",
    "        ci[index] = np.percentile(sampled_risks[index],[lower,upper])\n",
    "    return ci\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_clb = ci_calculator(clb_sampled_risks_array)\n",
    "df = pd.DataFrame(clb_treatment_array)\n",
    "df.to_csv('treatment_array_010_new.csv')\n",
    "\n",
    "data_export_clb = pd.DataFrame({'Percentage of patients': np.array(CLB_counts)/263,\n",
    "                                'T-stage': t_stage,\n",
    "                                'Midline Extension': midline_extension,\n",
    "                                'Involvement Ipsi' : involvement_ipsi_CLB,\n",
    "                                'Involvement Contra': involvement_contra_CLB,\n",
    "                                'Treated Ipsi':  clb_treated_ipsi,\n",
    "                                'Treated Contra': clb_treated_contra,\n",
    "                                'risk': clb_total_risks,\n",
    "                                'lower bound': ci_clb.T[0],\n",
    "                                'upper bound': ci_clb.T[1],\n",
    "                                'top 3 spared lnls risk': clb_top3_spared\n",
    "\n",
    "})\n",
    "data_export_clb.to_csv('analyzed_clb_data_010_new.csv', sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined USZ and CLB analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample array with different entry combinations\n",
    "data_full = np.array(analysis_full)\n",
    "\n",
    "entry_combinations_with_indexes = defaultdict(list)\n",
    "for index, row in enumerate(data_full):\n",
    "    combination = tuple(row)\n",
    "    entry_combinations_with_indexes[combination].append(index)\n",
    "full_counts = []\n",
    "full_combinations = []\n",
    "full_indexes = []\n",
    "# Print the most common combinations, their full_counts, and indexes\n",
    "for combination, indexes in entry_combinations_with_indexes.items():\n",
    "    count = len(indexes)\n",
    "    full_indexes.append(indexes)\n",
    "    # print(f\"Combination: {combination}, Count: {count}, Indexes: {indexes}\")\n",
    "    full_counts.append(count)\n",
    "    full_combinations.append(combination)\n",
    "\n",
    "lnls = ['I','II', 'III', 'IV','V', 'VII']\n",
    "t_stage = []\n",
    "midline_extension = []\n",
    "involvement_ipsi_full = []\n",
    "involvement_contra_full = []\n",
    "for diagnose_type in full_combinations:\n",
    "    involved_ipsi = []\n",
    "    involved_contra = []\n",
    "    t_stage.append(diagnose_type[0])\n",
    "    midline_extension.append(diagnose_type[1])\n",
    "    for lnl_looper, involved_level in enumerate(lnls):\n",
    "        if diagnose_type[lnl_looper +2] == True:\n",
    "            involved_ipsi.append(involved_level) \n",
    "        if diagnose_type[lnl_looper +8] == True:\n",
    "            involved_contra.append(involved_level)\n",
    "    involvement_ipsi_full.append(involved_ipsi)\n",
    "    involvement_contra_full.append(involved_contra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_treated_lnls_no_risk, full_treated_lnls_all, full_treatment_array, full_top3_spared, full_total_risks, full_treated_ipsi, full_treated_contra, full_sampled_risks_array = analysis_treated_lnls_sampled(full_combinations)\n",
    "full_set_counts = count_number_treatments(full_treated_lnls_no_risk)\n",
    "len(full_set_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_treated_ipsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ci_calculator(sampled_risks,level = 0.95):\n",
    "    lower = (1-level)/2*100\n",
    "    upper = 100- lower\n",
    "    ci = np.zeros((112,2))\n",
    "    for index in range(112):\n",
    "        ci[index] = np.percentile(sampled_risks[index],[lower,upper])\n",
    "    return ci\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_full = ci_calculator(full_sampled_risks_array)\n",
    "df = pd.DataFrame(full_treatment_array)\n",
    "df.to_csv('treatment_array_010_new.csv')\n",
    "\n",
    "data_export_full = pd.DataFrame({'Percentage of patients': np.array(full_counts)/550,\n",
    "                                'T-stage': t_stage,\n",
    "                                'Midline Extension': midline_extension,\n",
    "                                'Involvement Ipsi' : involvement_ipsi_full,\n",
    "                                'Involvement Contra': involvement_contra_full,\n",
    "                                'Treated Ipsi':  full_treated_ipsi,\n",
    "                                'Treated Contra': full_treated_contra,\n",
    "                                'risk': full_total_risks,\n",
    "                                'lower bound': ci_full.T[0],\n",
    "                                'upper bound': ci_full.T[1],\n",
    "                                'top 3 spared lnls risk': full_top3_spared\n",
    "\n",
    "})\n",
    "data_export_full.to_csv('analyzed_full_data_010_new.csv', sep = ';', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
